{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from Game import Game\n",
    "from Agent import Agent\n",
    "from GameState import GameState\n",
    "from DataLoader import DataLoader\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from IPython.display import clear_output\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#game parameters\n",
    "\n",
    "# possible actions: jump, do nothing\n",
    "ACTIONS = 2\n",
    "\n",
    "# decay rate of past observations original 0.99\n",
    "GAMMA = 0.99\n",
    "\n",
    "# timesteps to observe before training\n",
    "OBSERVATION = 100.\n",
    "\n",
    "# frames over which to anneal epsilon\n",
    "EXPLORE = 100000\n",
    "\n",
    "# final value of epsilon\n",
    "FINAL_EPSILON = 0.0001\n",
    "\n",
    "# starting value of epsilon\n",
    "INITIAL_EPSILON = 0.1\n",
    "\n",
    "# number of previous transitions to remember\n",
    "REPLAY_MEMORY = 50000\n",
    "\n",
    "# size of minibatch\n",
    "BATCH = 16\n",
    "\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "\n",
    "#We stack 4 frames\n",
    "img_channels = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Call only once to init file structure\n",
    "# data_loader.init_cache(INITIAL_EPSILON)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def buildmodel(data_loader: DataLoader):\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same', strides=(4, 4), input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(ACTIONS))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "    #create model file if not present\n",
    "    if not data_loader.is_loss_file_present():\n",
    "        model.save_weights('model.h5')\n",
    "\n",
    "    print(\"We finish building the model\")\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running offline..\n",
      "Now we build the model\n",
      "We finish building the model\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 64)          32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 112,290\n",
      "Trainable params: 112,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now we load weight\n",
      "Weight load successfully\n",
      "Jump\n",
      "fps: 0.14128987633720047\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.029446051312716045\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.018414075264399754\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.11345208314196106\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.7115113652055107\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.7014870480992232\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.7493240672091757\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "Jump\n",
      "fps: 0.7098139301972121\n"
     ]
    }
   ],
   "source": [
    "# main training module\n",
    "# Parameters:\n",
    "# * model => Keras Model to be trained\n",
    "# * game_state => Game State module with access to game environment and dino\n",
    "# * observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "def trainNetwork(model, game_state: GameState, data_loader: DataLoader, observe=False):\n",
    "    last_time = time.time()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = data_loader.load_obj(\"D\")\n",
    "\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "\n",
    "    #0 => do nothing,\n",
    "    #1 => jump\n",
    "    do_nothing[0] = 1\n",
    "\n",
    "    # get next step after performing the action\n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "\n",
    "    # stack 4 images to create placeholder input\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    #1*80*80*4\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "\n",
    "    initial_state = s_t\n",
    "\n",
    "    if observe :\n",
    "        #We keep observe, never train\n",
    "\n",
    "        OBSERVE = 999999999\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")\n",
    "    else:\n",
    "        #We go to training mode\n",
    "\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = data_loader.load_obj(\"epsilon\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    # resume from the previous time step stored in file system\n",
    "    t = data_loader.load_obj(\"time\")\n",
    "\n",
    "    #endless running\n",
    "    while True :\n",
    "\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "\n",
    "        # choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            #parameter to skip frames for actions\n",
    "\n",
    "            if  random.random() <= epsilon:\n",
    "                #randomly explore an action\n",
    "\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                # predict the output\n",
    "\n",
    "                # input a stack of 4 images, get the prediction\n",
    "                q = model.predict(s_t)\n",
    "\n",
    "                # choosing index with maximum q value\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "\n",
    "                # 0 => do nothing,\n",
    "                # 1 => jump\n",
    "                a_t[action_index] = 1\n",
    "\n",
    "        # We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "\n",
    "        # helpful for measuring frame rate\n",
    "        print('fps: {0}'.format(1 / (time.time() - last_time)))\n",
    "        last_time = time.time()\n",
    "\n",
    "        # 1x80x80x1\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)\n",
    "\n",
    "        # append the new image to input stack and remove the first one\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        # only train if done observing\n",
    "        if t > OBSERVE:\n",
    "\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            # Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                # 4D stack of images\n",
    "                state_t = minibatch[i][0]\n",
    "\n",
    "                # This is action index\n",
    "                action_t = minibatch[i][1]\n",
    "\n",
    "                # reward at state_t due to action_t\n",
    "                reward_t = minibatch[i][2]\n",
    "\n",
    "                # next state\n",
    "                state_t1 = minibatch[i][3]\n",
    "\n",
    "                # wheather the agent died or survided due the action\n",
    "                terminal = minibatch[i][4]\n",
    "\n",
    "                print('Agent State::', terminal)\n",
    "\n",
    "                inputs[i:i + 1] = state_t\n",
    "\n",
    "                # predicted q values\n",
    "                targets[i] = model.predict(state_t)\n",
    "\n",
    "                # predict q values for next step\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                # if terminated, only equals reward\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            data_loader.store_loss(loss)\n",
    "            data_loader.store_q_value(np.max(Q_sa))\n",
    "\n",
    "        #reset game to initial frame if terminate\n",
    "        s_t = initial_state if terminal else s_t1\n",
    "        t = t + 1\n",
    "\n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "\n",
    "            # pause game while saving to filesystem\n",
    "            game_state._game.pause()\n",
    "\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "\n",
    "            # saving episodes\n",
    "            data_loader.save_obj(D, \"D\")\n",
    "\n",
    "            # caching time steps\n",
    "            data_loader.save_obj(t,\"time\")\n",
    "\n",
    "            #cache epsilon to avoid repeated randomness in actions\n",
    "            data_loader.save_obj(epsilon,\"epsilon\")\n",
    "\n",
    "            data_loader.store_values_to_file()\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif OBSERVE < t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = Agent(game)\n",
    "    data_loader = DataLoader()\n",
    "    game_state = GameState(dino, game, data_loader)\n",
    "    model = buildmodel(data_loader)\n",
    "    try:\n",
    "        trainNetwork(model, game_state, data_loader, observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "playGame(observe=True);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-env-TM2020-py",
   "language": "python",
   "display_name": "Python [conda env:env-TM2020] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}