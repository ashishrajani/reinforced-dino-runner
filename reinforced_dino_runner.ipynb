{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from Game import Game\n",
    "from Agent import Agent\n",
    "from GameState import GameState\n",
    "from DataLoader import DataLoader\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from IPython.display import clear_output\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#game parameters\n",
    "\n",
    "# possible actions: jump, do nothing\n",
    "ACTIONS = 2\n",
    "\n",
    "# decay rate of past observations original 0.99\n",
    "GAMMA = 0.99\n",
    "\n",
    "# timesteps to observe before training\n",
    "OBSERVATION = 100.\n",
    "\n",
    "# frames over which to anneal epsilon\n",
    "EXPLORE = 100000\n",
    "\n",
    "# final value of epsilon\n",
    "FINAL_EPSILON = 0.0001\n",
    "\n",
    "# starting value of epsilon\n",
    "INITIAL_EPSILON = 0.1\n",
    "\n",
    "# number of previous transitions to remember\n",
    "REPLAY_MEMORY = 50000\n",
    "\n",
    "# size of minibatch\n",
    "BATCH = 16\n",
    "\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "\n",
    "#We stack 4 frames\n",
    "img_channels = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Call only once to init file structure\n",
    "# data_loader.init_cache(INITIAL_EPSILON)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def buildmodel(data_loader: DataLoader):\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same', strides=(4, 4), input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(ACTIONS))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "    #create model file if not present\n",
    "    if not data_loader.is_loss_file_present():\n",
    "        model.save_weights('model.h5')\n",
    "\n",
    "    print(\"We finish building the model\")\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# main training module\n",
    "# Parameters:\n",
    "# * model => Keras Model to be trained\n",
    "# * game_state => Game State module with access to game environment and dino\n",
    "# * observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "def trainNetwork(model, game_state: GameState, data_loader: DataLoader, observe=False):\n",
    "    last_time = time.time()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = data_loader.load_obj(\"D\")\n",
    "\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "\n",
    "    #0 => do nothing,\n",
    "    #1 => jump\n",
    "    do_nothing[0] = 1\n",
    "\n",
    "    # get next step after performing the action\n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "\n",
    "    # stack 4 images to create placeholder input\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    #1*80*80*4\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "\n",
    "    initial_state = s_t\n",
    "\n",
    "    if observe :\n",
    "        #We keep observe, never train\n",
    "\n",
    "        OBSERVE = 999999999\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")\n",
    "    else:\n",
    "        #We go to training mode\n",
    "\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = data_loader.load_obj(\"epsilon\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    # resume from the previous time step stored in file system\n",
    "    t = data_loader.load_obj(\"time\")\n",
    "\n",
    "    #endless running\n",
    "    while True :\n",
    "\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "\n",
    "        # choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            #parameter to skip frames for actions\n",
    "\n",
    "            if  random.random() <= epsilon:\n",
    "                #randomly explore an action\n",
    "\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                # predict the output\n",
    "\n",
    "                # input a stack of 4 images, get the prediction\n",
    "                q = model.predict(s_t)\n",
    "\n",
    "                # choosing index with maximum q value\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "\n",
    "                # 0 => do nothing,\n",
    "                # 1 => jump\n",
    "                a_t[action_index] = 1\n",
    "\n",
    "        # We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "\n",
    "        # helpful for measuring frame rate\n",
    "        print('fps: {0}'.format(1 / (time.time() - last_time)))\n",
    "        last_time = time.time()\n",
    "\n",
    "        # 1x80x80x1\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)\n",
    "\n",
    "        # append the new image to input stack and remove the first one\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        # only train if done observing\n",
    "        if t > OBSERVE:\n",
    "\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            # Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                # 4D stack of images\n",
    "                state_t = minibatch[i][0]\n",
    "\n",
    "                # This is action index\n",
    "                action_t = minibatch[i][1]\n",
    "\n",
    "                # reward at state_t due to action_t\n",
    "                reward_t = minibatch[i][2]\n",
    "\n",
    "                # next state\n",
    "                state_t1 = minibatch[i][3]\n",
    "\n",
    "                # wheather the agent died or survided due the action\n",
    "                terminal = minibatch[i][4]\n",
    "\n",
    "                print('Agent State::', terminal)\n",
    "\n",
    "                inputs[i:i + 1] = state_t\n",
    "\n",
    "                # predicted q values\n",
    "                targets[i] = model.predict(state_t)\n",
    "\n",
    "                # predict q values for next step\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                # if terminated, only equals reward\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            data_loader.store_loss(loss)\n",
    "            data_loader.store_q_value(np.max(Q_sa))\n",
    "\n",
    "        #reset game to initial frame if terminate\n",
    "        s_t = initial_state if terminal else s_t1\n",
    "        t = t + 1\n",
    "\n",
    "        # save progress every 1000 iterations\n",
    "        if t % 10 == 0:\n",
    "            print(\"Now we save model\")\n",
    "\n",
    "            # pause game while saving to filesystem\n",
    "            game_state._game.pause()\n",
    "\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "\n",
    "            # saving episodes\n",
    "            data_loader.save_obj(D, \"D\")\n",
    "\n",
    "            # caching time steps\n",
    "            data_loader.save_obj(t,\"time\")\n",
    "\n",
    "            #cache epsilon to avoid repeated randomness in actions\n",
    "            data_loader.save_obj(epsilon,\"epsilon\")\n",
    "\n",
    "            data_loader.store_values_to_file()\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif OBSERVE < t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = Agent(game)\n",
    "    data_loader = DataLoader()\n",
    "    game_state = GameState(dino, game, data_loader)\n",
    "    model = buildmodel(data_loader)\n",
    "    try:\n",
    "        trainNetwork(model, game_state, data_loader, observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 4860 / STATE explore / EPSILON 0.09524575899999274 / ACTION 1 / REWARD 0.1 / Q_MAX  31.831926 / Loss  0.47312474250793457\n",
      "Jump\n",
      "fps: 2.5193299812775813\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4861 / STATE explore / EPSILON 0.09524475999999274 / ACTION 1 / REWARD 0.1 / Q_MAX  30.410334 / Loss  4.748752117156982\n",
      "fps: 6.276051431760743\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4862 / STATE explore / EPSILON 0.09524376099999274 / ACTION 0 / REWARD 0.1 / Q_MAX  28.168766 / Loss  1.795883059501648\n",
      "fps: 5.699200756577249\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: True\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4863 / STATE explore / EPSILON 0.09524276199999274 / ACTION 0 / REWARD 0.1 / Q_MAX  30.373377 / Loss  4.9499192237854\n",
      "fps: 6.452954081805342\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4864 / STATE explore / EPSILON 0.09524176299999274 / ACTION 0 / REWARD 0.1 / Q_MAX  31.630173 / Loss  1.2632474899291992\n",
      "fps: 6.676999293187216\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4865 / STATE explore / EPSILON 0.09524076399999273 / ACTION 0 / REWARD 0.1 / Q_MAX  29.556574 / Loss  0.7132024168968201\n",
      "fps: 6.653401015228426\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "Agent State:: False\n",
      "TIMESTEP 4866 / STATE explore / EPSILON 0.09523976499999273 / ACTION 0 / REWARD 0.1 / Q_MAX  29.231936 / Loss  2.4511783123016357\n",
      "----------Random Action----------\n",
      "Jump\n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=89.0.4389.114)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mElementNotInteractableException\u001B[0m           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-4829a0a02061>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mplayGame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobserve\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-7-ad80a1b43e4a>\u001B[0m in \u001B[0;36mplayGame\u001B[0;34m(observe)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuildmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0mtrainNetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgame_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobserve\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mobserve\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-65e4dd84e38c>\u001B[0m in \u001B[0;36mtrainNetwork\u001B[0;34m(model, game_state, data_loader, observe)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0;31m# run the selected action and observed next state and reward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m         \u001B[0mx_t1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mr_t\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mterminal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgame_state\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_t\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m         \u001B[0;31m# helpful for measuring frame rate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Personal Projects/Reinforced Dino Runner/GameState.py\u001B[0m in \u001B[0;36mget_state\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mactions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Jump'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_agent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_grab_screen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_driver\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Personal Projects/Reinforced Dino Runner/Agent.py\u001B[0m in \u001B[0;36mjump\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;31m# make agent jump\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpress_up\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mduck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Personal Projects/Reinforced Dino Runner/Game.py\u001B[0m in \u001B[0;36mpress_up\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     59\u001B[0m         \u001B[0;31m# sends a single to press up get to the browser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_driver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_element_by_tag_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"body\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_keys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mKeys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mARROW_UP\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpress_down\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001B[0m in \u001B[0;36msend_keys\u001B[0;34m(self, *value)\u001B[0m\n\u001B[1;32m    477\u001B[0m         self._execute(Command.SEND_KEYS_TO_ELEMENT,\n\u001B[1;32m    478\u001B[0m                       {'text': \"\".join(keys_to_typing(value)),\n\u001B[0;32m--> 479\u001B[0;31m                        'value': keys_to_typing(value)})\n\u001B[0m\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# RenderedWebElement Items\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001B[0m in \u001B[0;36m_execute\u001B[0;34m(self, command, params)\u001B[0m\n\u001B[1;32m    631\u001B[0m             \u001B[0mparams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    632\u001B[0m         \u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 633\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_parent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    634\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    635\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfind_element\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mby\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mID\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, driver_command, params)\u001B[0m\n\u001B[1;32m    319\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcommand_executor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdriver_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    320\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 321\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_response\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    322\u001B[0m             response['value'] = self._unwrap_value(\n\u001B[1;32m    323\u001B[0m                 response.get('value', None))\n",
      "\u001B[0;32m/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001B[0m in \u001B[0;36mcheck_response\u001B[0;34m(self, response)\u001B[0m\n\u001B[1;32m    240\u001B[0m                 \u001B[0malert_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'alert'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mexception_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscreen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstacktrace\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malert_text\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mexception_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscreen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstacktrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_value_or_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mElementNotInteractableException\u001B[0m: Message: element not interactable\n  (Session info: chrome=89.0.4389.114)\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-env-TM2020-py",
   "language": "python",
   "display_name": "Python [conda env:env-TM2020] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}